{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "from FileReader import get_picture_tensors\n",
    "from ModelEvaluation import eval_model\n",
    "\n",
    "from CatNet.datasets import dataset_cat_no_cat\n",
    "from CatNet.models import CatNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 112, 112]             864\n",
      "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
      "             ReLU6-3         [-1, 32, 112, 112]               0\n",
      "            Conv2d-4         [-1, 32, 112, 112]             288\n",
      "       BatchNorm2d-5         [-1, 32, 112, 112]              64\n",
      "             ReLU6-6         [-1, 32, 112, 112]               0\n",
      "            Conv2d-7         [-1, 16, 112, 112]             512\n",
      "       BatchNorm2d-8         [-1, 16, 112, 112]              32\n",
      "  InvertedResidual-9         [-1, 16, 112, 112]               0\n",
      "           Conv2d-10         [-1, 96, 112, 112]           1,536\n",
      "      BatchNorm2d-11         [-1, 96, 112, 112]             192\n",
      "            ReLU6-12         [-1, 96, 112, 112]               0\n",
      "           Conv2d-13           [-1, 96, 56, 56]             864\n",
      "      BatchNorm2d-14           [-1, 96, 56, 56]             192\n",
      "            ReLU6-15           [-1, 96, 56, 56]               0\n",
      "           Conv2d-16           [-1, 24, 56, 56]           2,304\n",
      "      BatchNorm2d-17           [-1, 24, 56, 56]              48\n",
      " InvertedResidual-18           [-1, 24, 56, 56]               0\n",
      "           Conv2d-19          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-20          [-1, 144, 56, 56]             288\n",
      "            ReLU6-21          [-1, 144, 56, 56]               0\n",
      "           Conv2d-22          [-1, 144, 56, 56]           1,296\n",
      "      BatchNorm2d-23          [-1, 144, 56, 56]             288\n",
      "            ReLU6-24          [-1, 144, 56, 56]               0\n",
      "           Conv2d-25           [-1, 24, 56, 56]           3,456\n",
      "      BatchNorm2d-26           [-1, 24, 56, 56]              48\n",
      " InvertedResidual-27           [-1, 24, 56, 56]               0\n",
      "           Conv2d-28          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-29          [-1, 144, 56, 56]             288\n",
      "            ReLU6-30          [-1, 144, 56, 56]               0\n",
      "           Conv2d-31          [-1, 144, 28, 28]           1,296\n",
      "      BatchNorm2d-32          [-1, 144, 28, 28]             288\n",
      "            ReLU6-33          [-1, 144, 28, 28]               0\n",
      "           Conv2d-34           [-1, 32, 28, 28]           4,608\n",
      "      BatchNorm2d-35           [-1, 32, 28, 28]              64\n",
      " InvertedResidual-36           [-1, 32, 28, 28]               0\n",
      "           Conv2d-37          [-1, 192, 28, 28]           6,144\n",
      "      BatchNorm2d-38          [-1, 192, 28, 28]             384\n",
      "            ReLU6-39          [-1, 192, 28, 28]               0\n",
      "           Conv2d-40          [-1, 192, 28, 28]           1,728\n",
      "      BatchNorm2d-41          [-1, 192, 28, 28]             384\n",
      "            ReLU6-42          [-1, 192, 28, 28]               0\n",
      "           Conv2d-43           [-1, 32, 28, 28]           6,144\n",
      "      BatchNorm2d-44           [-1, 32, 28, 28]              64\n",
      " InvertedResidual-45           [-1, 32, 28, 28]               0\n",
      "           Conv2d-46          [-1, 192, 28, 28]           6,144\n",
      "      BatchNorm2d-47          [-1, 192, 28, 28]             384\n",
      "            ReLU6-48          [-1, 192, 28, 28]               0\n",
      "           Conv2d-49          [-1, 192, 28, 28]           1,728\n",
      "      BatchNorm2d-50          [-1, 192, 28, 28]             384\n",
      "            ReLU6-51          [-1, 192, 28, 28]               0\n",
      "           Conv2d-52           [-1, 32, 28, 28]           6,144\n",
      "      BatchNorm2d-53           [-1, 32, 28, 28]              64\n",
      " InvertedResidual-54           [-1, 32, 28, 28]               0\n",
      "           Conv2d-55          [-1, 192, 28, 28]           6,144\n",
      "      BatchNorm2d-56          [-1, 192, 28, 28]             384\n",
      "            ReLU6-57          [-1, 192, 28, 28]               0\n",
      "           Conv2d-58          [-1, 192, 14, 14]           1,728\n",
      "      BatchNorm2d-59          [-1, 192, 14, 14]             384\n",
      "            ReLU6-60          [-1, 192, 14, 14]               0\n",
      "           Conv2d-61           [-1, 64, 14, 14]          12,288\n",
      "      BatchNorm2d-62           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-63           [-1, 64, 14, 14]               0\n",
      "           Conv2d-64          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-65          [-1, 384, 14, 14]             768\n",
      "            ReLU6-66          [-1, 384, 14, 14]               0\n",
      "           Conv2d-67          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-68          [-1, 384, 14, 14]             768\n",
      "            ReLU6-69          [-1, 384, 14, 14]               0\n",
      "           Conv2d-70           [-1, 64, 14, 14]          24,576\n",
      "      BatchNorm2d-71           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-72           [-1, 64, 14, 14]               0\n",
      "           Conv2d-73          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-74          [-1, 384, 14, 14]             768\n",
      "            ReLU6-75          [-1, 384, 14, 14]               0\n",
      "           Conv2d-76          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-77          [-1, 384, 14, 14]             768\n",
      "            ReLU6-78          [-1, 384, 14, 14]               0\n",
      "           Conv2d-79           [-1, 64, 14, 14]          24,576\n",
      "      BatchNorm2d-80           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-81           [-1, 64, 14, 14]               0\n",
      "           Conv2d-82          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-83          [-1, 384, 14, 14]             768\n",
      "            ReLU6-84          [-1, 384, 14, 14]               0\n",
      "           Conv2d-85          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-86          [-1, 384, 14, 14]             768\n",
      "            ReLU6-87          [-1, 384, 14, 14]               0\n",
      "           Conv2d-88           [-1, 64, 14, 14]          24,576\n",
      "      BatchNorm2d-89           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-90           [-1, 64, 14, 14]               0\n",
      "           Conv2d-91          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-92          [-1, 384, 14, 14]             768\n",
      "            ReLU6-93          [-1, 384, 14, 14]               0\n",
      "           Conv2d-94          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-95          [-1, 384, 14, 14]             768\n",
      "            ReLU6-96          [-1, 384, 14, 14]               0\n",
      "           Conv2d-97           [-1, 96, 14, 14]          36,864\n",
      "      BatchNorm2d-98           [-1, 96, 14, 14]             192\n",
      " InvertedResidual-99           [-1, 96, 14, 14]               0\n",
      "          Conv2d-100          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-101          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-102          [-1, 576, 14, 14]               0\n",
      "          Conv2d-103          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-104          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-105          [-1, 576, 14, 14]               0\n",
      "          Conv2d-106           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-107           [-1, 96, 14, 14]             192\n",
      "InvertedResidual-108           [-1, 96, 14, 14]               0\n",
      "          Conv2d-109          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-110          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-111          [-1, 576, 14, 14]               0\n",
      "          Conv2d-112          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-113          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-114          [-1, 576, 14, 14]               0\n",
      "          Conv2d-115           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-116           [-1, 96, 14, 14]             192\n",
      "InvertedResidual-117           [-1, 96, 14, 14]               0\n",
      "          Conv2d-118          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-119          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-120          [-1, 576, 14, 14]               0\n",
      "          Conv2d-121            [-1, 576, 7, 7]           5,184\n",
      "     BatchNorm2d-122            [-1, 576, 7, 7]           1,152\n",
      "           ReLU6-123            [-1, 576, 7, 7]               0\n",
      "          Conv2d-124            [-1, 160, 7, 7]          92,160\n",
      "     BatchNorm2d-125            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-126            [-1, 160, 7, 7]               0\n",
      "          Conv2d-127            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-128            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-129            [-1, 960, 7, 7]               0\n",
      "          Conv2d-130            [-1, 960, 7, 7]           8,640\n",
      "     BatchNorm2d-131            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-132            [-1, 960, 7, 7]               0\n",
      "          Conv2d-133            [-1, 160, 7, 7]         153,600\n",
      "     BatchNorm2d-134            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-135            [-1, 160, 7, 7]               0\n",
      "          Conv2d-136            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-137            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-138            [-1, 960, 7, 7]               0\n",
      "          Conv2d-139            [-1, 960, 7, 7]           8,640\n",
      "     BatchNorm2d-140            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-141            [-1, 960, 7, 7]               0\n",
      "          Conv2d-142            [-1, 160, 7, 7]         153,600\n",
      "     BatchNorm2d-143            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-144            [-1, 160, 7, 7]               0\n",
      "          Conv2d-145            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-146            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-147            [-1, 960, 7, 7]               0\n",
      "          Conv2d-148            [-1, 960, 7, 7]           8,640\n",
      "     BatchNorm2d-149            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-150            [-1, 960, 7, 7]               0\n",
      "          Conv2d-151            [-1, 320, 7, 7]         307,200\n",
      "     BatchNorm2d-152            [-1, 320, 7, 7]             640\n",
      "InvertedResidual-153            [-1, 320, 7, 7]               0\n",
      "          Conv2d-154           [-1, 1280, 7, 7]         409,600\n",
      "     BatchNorm2d-155           [-1, 1280, 7, 7]           2,560\n",
      "           ReLU6-156           [-1, 1280, 7, 7]               0\n",
      "AdaptiveAvgPool2d-157           [-1, 1280, 1, 1]               0\n",
      "         Dropout-158                 [-1, 1280]               0\n",
      "          Linear-159                    [-1, 1]           1,281\n",
      "================================================================\n",
      "Total params: 2,225,153\n",
      "Trainable params: 1,281\n",
      "Non-trainable params: 2,223,872\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 152.87\n",
      "Params size (MB): 8.49\n",
      "Estimated Total Size (MB): 161.93\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "n_classes = 32\n",
    "\n",
    "# m√©canisme d'attention = 1 sortie (0 c'est background, 1 c'est chat)\n",
    "model = CatNet(cnn_backbone = 'mobilenet_v2', num_classes = 1)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "loss_at_each_epoch = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data params\n",
    "# cat_directory = \"normal_prep_datasets/dataset_chat_downscale_no_background/\"\n",
    "# background_directory = \"normal_prep_datasets/dataset_chat_downscale_no_cat\"\n",
    "\n",
    "cat_directory = \"dataset_augmented_no_background\"\n",
    "background_directory = \"dataset_augmented_no_cat\"\n",
    "\n",
    "required_train_imgs = 10\n",
    "required_test_imgs = 1\n",
    "\n",
    "num_epochs = 32\n",
    "# batches_per_epoch = 8\n",
    "batches_per_epoch = 32\n",
    "\n",
    "batch_size = 8\n",
    "# batch_size = 128\n",
    "\n",
    "# learning_rate = 1e-5\n",
    "learning_rate = 1e-4\n",
    "# learning_rate = 1e-3\n",
    "# learning_rate = 1e-2\n",
    "# learning_rate = 1e-6\n",
    "\n",
    "# ratio = [0.75, 0.25]\n",
    "ratio = 0.5\n",
    "\n",
    "model.train()\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "\n",
    "params = filter(lambda x: x.requires_grad, model.parameters())\n",
    "optimizer = optim.Adam(params, lr = learning_rate)\n",
    "\n",
    "# # Gossage sur les params potentiel\n",
    "# momentum = 0.5\n",
    "# optimizer = optim.SGD(params, lr = learning_rate, momentum = momentum)   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    (train_images_cat, val_images_cat, test_images_cat, \n",
    "    train_labels_cat , val_labels_cat, test_labels_cat, n_classes) = get_picture_tensors(root_directory=cat_directory,\n",
    "                                                                n_classes=n_classes, \n",
    "                                                                required_train_imgs=required_train_imgs, \n",
    "                                                                required_test_imgs=required_test_imgs,\n",
    "                                                                use_selected_eval_datasets = False,\n",
    "                                                                shuffle_directories = True,\n",
    "                                                                shuffle_images = True, \n",
    "                                                                show_progress=False,\n",
    "                                                                ordered_dataset=True)\n",
    "    \n",
    "\n",
    "    (train_images_background, val_images_background, test_images_background, \n",
    "    train_labels_background , val_labels_background, test_labels_background7 , n_classes) = get_picture_tensors(root_directory=background_directory,\n",
    "                                                                n_classes=n_classes, \n",
    "                                                                required_train_imgs=required_train_imgs, \n",
    "                                                                required_test_imgs=required_test_imgs,\n",
    "                                                                use_selected_eval_datasets = False,\n",
    "                                                                shuffle_directories = True,\n",
    "                                                                shuffle_images = True, \n",
    "                                                                show_progress=False,\n",
    "                                                                ordered_dataset=True)\n",
    "\n",
    "    train_dataset = dataset_cat_no_cat(train_images_cat, train_images_background)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "    total_loss = 0\n",
    "    \n",
    "    for imgs, labels in train_dataloader:\n",
    "        # # Entrainement de l'auto-encodeur\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print(labels)\n",
    "        # print(imgs.shape)\n",
    "        # print(imgs.shape)\n",
    "\n",
    "        output = model(imgs)\n",
    "        # print(output)\n",
    "\n",
    "        # output = torch.flatten(torch.sigmoid(output))  # sigmoid ici si BCEloss, sinon pas besoin car inclut dans bcewithlogitloss\n",
    "        output = torch.flatten(output)\n",
    "\n",
    "\n",
    "        # print(output)\n",
    "        loss = criterion(output, labels.float()) \n",
    "\n",
    "        total_loss += loss.detach().numpy()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    print(f\"End of epoch {epoch}\")\n",
    "    print(\"Total loss in epoch: \", total_loss)\n",
    "\n",
    "    loss_at_each_epoch.append(total_loss)\n",
    "\n",
    "\n",
    "\n",
    "    # maxtot = max([max(loss_at_each_epoch), max(loss_at_each_epoch_label0), max(loss_at_each_epoch_label1)])\n",
    "    # max_autoencodeur = max(loss_at_each_epoch_autoencodeur)\n",
    "    # max_similarite = max(loss_at_each_epoch_similarite)\n",
    "\n",
    "    # maxtot = max([max_autoencodeur, max_similarite])\n",
    "    maxtot = max(loss_at_each_epoch)\n",
    "\n",
    "    from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 2.5))\n",
    "\n",
    "    ax.set_xlabel('Epoch (-)')\n",
    "    # ax.set_ylabel('Validation accuracy (%)')\n",
    "    ax.set_ylabel('Loss (-)')\n",
    "    # ax.set_ylim(0, 100)\n",
    "    # ax.set_ylim(0, 7500)\n",
    "    ax.set_ylim(0, 1.05*maxtot)\n",
    "    # ax.set_yticks(np.arange(0, 110, 10))\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
    "\n",
    "    ax.plot(loss_at_each_epoch, '.-')\n",
    "    # ax.plot(loss_at_each_epoch_similarite, '.-', label='similarite')\n",
    "    # ax.plot(loss_at_each_epoch_label0, '.-', label='Diff cats')\n",
    "    # ax.plot(loss_at_each_epoch_label1, '.-', label='Same cat')\n",
    "\n",
    "    # ax.legend(loc='upper right')\n",
    "    # ax.legend(loc='best')\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_param = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "        print(param.data)\n",
    "        all_param.append(param.data)\n",
    "    \n",
    "print(model.classifier[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = f\"{model.cnn_backbone}_attention\"\n",
    "\n",
    "# # maxtot = max([max_autoencodeur, max_similarite])\n",
    "# maxtot = max(loss_at_each_epoch)\n",
    "\n",
    "# from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 2.5))\n",
    "\n",
    "# ax.set_xlabel('Epoch (-)')\n",
    "# ax.set_ylabel('Validation accuracy (%)')\n",
    "# ax.set_ylabel('Loss (-)')\n",
    "# ax.set_ylim(0, 100)\n",
    "# ax.set_ylim(0, 7500)\n",
    "# ax.set_ylim(0, 1.05*maxtot)\n",
    "# ax.set_yticks(np.arange(0, 110, 10))\n",
    "# ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
    "\n",
    "print(all_param[0].shape)\n",
    "\n",
    "y = torch.flatten(all_param[0])\n",
    "\n",
    "ax.plot(np.arange(0, len(y)), y, '.')\n",
    "# ax.plot(loss_at_each_epoch_similarite, '.-', label='similarite')\n",
    "# ax.plot(loss_at_each_epoch_label0, '.-', label='Diff cats')\n",
    "# ax.plot(loss_at_each_epoch_label1, '.-', label='Same cat')\n",
    "\n",
    "# ax.legend(loc='upper right')\n",
    "# ax.legend(loc='best')\n",
    "\n",
    "# plt.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# fig.savefig(f\"{filename}.png\", dpi = 300)\n",
    "# fig.savefig(f\"{filename}.svg\", dpi = 300)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model.classifier[-1].state_dict(), 'mobilenetv2_attentionlayer_augmented.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"{model.cnn_backbone}_attention\"\n",
    "\n",
    "# maxtot = max([max_autoencodeur, max_similarite])\n",
    "maxtot = max(loss_at_each_epoch)\n",
    "\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 2.5))\n",
    "\n",
    "ax.set_xlabel('Epoch (-)')\n",
    "# ax.set_ylabel('Validation accuracy (%)')\n",
    "ax.set_ylabel('Loss (-)')\n",
    "# ax.set_ylim(0, 100)\n",
    "# ax.set_ylim(0, 7500)\n",
    "ax.set_ylim(0, 1.05*maxtot)\n",
    "# ax.set_yticks(np.arange(0, 110, 10))\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
    "\n",
    "ax.plot(loss_at_each_epoch, '.-')\n",
    "# ax.plot(loss_at_each_epoch_similarite, '.-', label='similarite')\n",
    "# ax.plot(loss_at_each_epoch_label0, '.-', label='Diff cats')\n",
    "# ax.plot(loss_at_each_epoch_label1, '.-', label='Same cat')\n",
    "\n",
    "# ax.legend(loc='upper right')\n",
    "# ax.legend(loc='best')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(f\"{filename}.png\", dpi = 300)\n",
    "fig.savefig(f\"{filename}.svg\", dpi = 300)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# model.save_parameters_to_file('mobilenetv2_attention.pth')\n",
    "model.save_parameters_to_file(f\"{filename}.pth\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CatProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
